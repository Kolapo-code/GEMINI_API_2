# -*- coding: utf-8 -*-
"""Copy of Empty_Gemini_Final_Exercise.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16NhOPnBT4axEmgHnBh-Zbe7juukeBWKA
"""

!apt install tesseract-ocr libtesseract-dev
!pip install -q -U google-generativeai chromadb pytesseract

import time
from tqdm import tqdm
import pathlib
import google.generativeai as genai
from google.generativeai import GenerativeModel
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import pandas as pd
from PIL import Image
import pytesseract
from IPython.display import Markdown

from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

genai.configure(api_key=GOOGLE_API_KEY)

"""# Gemini Final Exercise

You're an astronomy student who's very curious about the Apollo 11 missions,
and through your research, you've found a lot of different types of data (otherwise known as multimodal) from NASA's
public archive.

1. Text: You have the full final NASA report post-mission, spanning over 300
pages of incredibly informative content that details a summary of everything
that happened as well as conclusions that NASA researchers and engineers
came to. For the sake of this exercise, we've selected 3 particularly interesting pages, and converted them to images (you'll see soon why).

2. Video: You also have several clips of the famous Neil Armstrong and Buzz Aldrin footage as they
first stepped onto the moon, containing highlights of their moonwalks as well
as raising the American flag.

3. Audio: Finally, you have highlights from the audio recorded throughout the
mission, which provides insights into how communication between the astronauts
occurred as well as from the astronauts to mission control.

Now, you want to search through and summarize this information for your
upcoming research paper. Using your newfound skills from this course, you
can accomplish this using Gemini! In particular, we will build a Retrieval Augmented Generation (RAG) system that you can directly interact with.

## Data Preparation

Before we begin, ensure that you've uploaded the resources.zip folder and unzipped it using the following command:
"""

!wget -O resources.zip "https://video.udacity-data.com/topher/2024/June/66744e79_resources/resources.zip"

!unzip resources.zip

"""
As we saw throughout this course, when working with different types of data,
we first need to parse it in a way that Gemini can understand. We will prepare our data by extracting all file names from the `resources` directory."""

data_dir = pathlib.Path("resources/")
all_file_names = [str(file) for file in data_dir.rglob("*") if file.is_file() and not file.name.startswith('.')]

for file_name in all_file_names:
    print(file_name)

print(len(all_file_names))

"""You should expect to see 14 files.

## Retrieval Augmented Generation (RAG)

To showcase how we build a RAG, we will first build one for the Text case, and generalize it further after. Here is the general idea:
1. **Data Preparation** (done above): We first collected various types of data from NASA's public archive related to the Apollo 11 mission, including text, video, and audio files.
2. **Data Extraction and Summarization**: Extract the multimodal data from images, e.g. extract text from images using Optical Character Recognition (OCR), and use Gemini to generate summaries using a specialized prompt.
3. **Embedding Generation**: Convert the generated summaries into vector embeddings using Gemini's Text Embedding Model. These embeddings represent the summaries in a numerical format suitable for efficient similarity searches.
4. **Creating a Vector Database**: A Vector database was created to store the embeddings. This database facilitates fast and efficient retrieval of relevant documents based on similarity searches. We chose to use Chroma DB.
5. **Querying the RAG System**: For a given query, the system retrieves the most relevant documents (based on their embeddings) and generates a response using the retrieved documents as context.

Something important to note is that RAGs are usually used only when there is a surplus of data. In other words, if the data can't fit into the model prompt. In this case, the data we provided likely can fit into Gemini's 1 million token window, but for the sake of simplicity and restrictions of Google Colab's runtime, we opted to use a smaller set of data.

### Text

We will use Tesseract OCR (Optical Character Recognition) to extract text from images of the NASA report.
"""

pytesseract.pytesseract.tesseract_cmd = (r'/usr/bin/tesseract')

"""Let's create a function to take in our images of a PDF, transcribe them into text, and summarize each of them."""

def create_text_summary(model):
    path = pathlib.Path("resources/text")

    images = []
    text_summaries = []

    for f in path.glob("*"):
        if f.is_dir() or f.name.startswith('.'):
            continue

        try:
            # Open the image file
            image = Image.open(f)

            # Perform OCR to extract text from image
            content = pytesseract.image_to_string(image)

            # Generate the summary using the model
            response = model.generate_content(content)  # Adjust method name and arguments as per your model

            # Append the image and the generated summary to the lists
            images.append(image)
            text_summaries.append(response.text)

        except UnicodeDecodeError:
            # Handle encoding issues here, e.g., try different encodings or skip problematic files
            print(f"Error decoding {f}")
            continue

    return images, text_summaries

# Example usage:
# Initialize GenerativeModel according to its expected parameters
model = GenerativeModel()  # Adjust initialization based on your setup

safety_settings = [
    {
        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
        "threshold": "BLOCK_NONE",
    },
]

model = genai.GenerativeModel('models/gemini-1.5-flash', safety_settings=safety_settings)

image_files, text_summaries = create_text_summary(model)

print(dir(model))

"""Now, we can check out the generated summaries of the three pages we have!"""

for text_summary in text_summaries:
  print(text_summary)

"""We create the Chroma database using the generated summaries. You might be wondering what Vector DB and Chroma DB are.

**Vector Database**: A specialized database designed to store and manage high-dimensional vectors, which are numerical representations of data points. It allows efficient similarity searches to find vectors (and their corresponding data) that are close to a given query vector.

**Chroma DB**: An implementation of a vector database used to store and retrieve vector embeddings. These embeddings are generated from our summaries and allow us to perform efficient similarity searches.

"""

class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        model = 'models/text-embedding-004'
        title = "Custom query"

        # Assuming `input` contains the documents or text segments to embed
        content = [doc.text for doc in input]  # Example: Extracting text from Documents object

        # Determine the appropriate task_type based on your embedding needs
        task_type = "semantic_similarity"  # Adjust based on Gemini documentation

        # Embed the content using genai library
        embedding = genai.embed_content(model=model, content=content, task_type=task_type, title=title)["embedding"]

        return Embeddings(embedding=embedding)

import chromadb

def create_chroma_db(documents, name):
    # Initialize ChromaDB client
    chroma_client = chromadb.Client()

    # Initialize your embedding function (GeminiEmbeddingFunction assumed here)
    embedding_function = GeminiEmbeddingFunction()

    # Create or get ChromaDB collection
    db = chroma_client.get_or_create_collection(
        name=name,
        embedding_function=embedding_function
    )

    # Add documents to the ChromaDB collection
    for i, d in enumerate(documents):
        db.add(
            document=d,
            id=str(i)
        )

    return db

def create_chroma_db(documents, name):
    # Initialize ChromaDB client
    chroma_client = chromadb.Client()

    # Initialize your embedding function (GeminiEmbeddingFunction assumed here)
    embedding_function = GeminiEmbeddingFunction()

    # Create or get ChromaDB collection
    db = chroma_client.get_or_create_collection(
        name=name,
        embedding_function=embedding_function
    )

    # Add documents to the ChromaDB collection
    for i, d in enumerate(documents):
        db.add(
            text=d["text"],  # Assuming each document is a dictionary with a "text" key
            id=str(i)
        )

    return db

"""Let's also take a peak at the `text_db` and ensure that embeddings were generated:"""

from chromadb import Client, Documents, EmbeddingFunction, Embeddings
import pandas as pd

# Define EmbeddingTaskType with required constants
class EmbeddingTaskType:
    TASK_TYPE_UNSPECIFIED = 0
    RETRIEVAL_QUERY = 1
    RETRIEVAL_DOCUMENT = 2
    SEMANTIC_SIMILARITY = 3
    CLASSIFICATION = 4
    CLUSTERING = 5
    QUESTION_ANSWERING = 6
    FACT_VERIFICATION = 7

# Define the embedding function
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        model = 'models/text-embedding-004'
        title = "Custom query"
        return genai.embed_content(model=model,
                                   content=input,  # Passing the input directly here
                                   task_type=EmbeddingTaskType.RETRIEVAL_DOCUMENT,  # Use the valid task type
                                   title=title)["embedding"]

# Define the function to create ChromaDB collection
def create_chroma_db(documents, name):
    chroma_client = Client()
    embedding_function = GeminiEmbeddingFunction()

    # Create or get the collection
    db = chroma_client.get_or_create_collection(name=name, embedding_function=embedding_function)

    # Add documents to the ChromaDB collection
    for i, d in enumerate(documents):
        db.add(
            documents=[d],
            ids=[str(i)]
        )

    return db

# Example documents to be added (replace with actual summaries)
documents = ["Summary 1", "Summary 2", "Summary 3"]

# Create ChromaDB collection for text summaries
text_db = create_chroma_db(documents, name="text_summaries")

# Retrieve embeddings and documents from text_db
embeddings = text_db.peek()['embeddings']
documents = text_db.peek()['documents']

# Construct a DataFrame
data = {'embeddings': embeddings, 'documents': documents}
df = pd.DataFrame.from_dict(data)

# Display the DataFrame
print(df.head())  # Displaying the first few rows for demonstration

from google.generativeai.embedding import EmbeddingTaskType

print(EmbeddingTaskType.__members__)

"""You should see a column called `embeddings` with what are seemingly random values, but these values are actually high-dimensional vectors that represent the semantic meaning of your summaries.

Now let's actually try querying our information. We'll test a simple example like getting some file that has to do with the Apollo 11 Flight Plan.
"""

def get_relevant_files(query, db, top_k=5):
    # Perform the query to get the top K relevant results
    results = db.query(
        query_texts=[query],
        n_results=top_k
    )

    # Print the results to understand their structure
    print(results)

    # Extract the documents directly from the query results
    relevant_files = results['documents'][0]  # The documents are in a nested list

    return relevant_files

# Example usage:
query = "Sample query text"
relevant_files = get_relevant_files(query, text_db, top_k=5)

# Print out the relevant files
print(relevant_files)

files = get_relevant_files("Apollo 11 Flight Plan", text_db)
print(files)

"""You should expect to see something like `['1', '0', '2']`. This means that the first entry in the `text_db` is most similar. If you look above at our `pd.DataFrame` output, the document with id 1 is the document about the Apollo 11 Flight Plan, so this is working as we expected!

### Video and Audio

Congrats! You've successfully built a working RAG for text. Now, let's extend this concept to Video and Audio, and build out some more complex queries. We'll begin by generalizing the above summary creation function to all sorts of modalities.
"""

def create_summary(modality):
  path = data_dir / modality

  summary_prompt = f"""You are an assistant tailored for summarizing {modality} for retrieval.
  These summaries will be turned into vector embeddings and used to retrieve the raw {modality}.
  Give a concise summary of the {modality} that is well optimized for retrieval. Here is the {modality}."""

  files = []
  summaries = []

  for f in path.glob("*"):
    if f.is_dir() or f.name.startswith('.'):
      continue
    print(f)

    if modality == "text":
      file = Image.open(f)
      response = model.generate_content([summary_prompt, pytesseract.image_to_string(file)])

    else:
      file = genai.upload_file(f)

      while file.state.name == "PROCESSING":
        print("Waiting for video file upload...\n", end='')
        time.sleep(5)
        file = genai.get_file(file.name)

      response = model.generate_content([summary_prompt, file])

    files.append(file)
    summaries.append(response.text)

  return files, summaries

!pip list

"""Now, we will create a folder with all of our data of different modalities. In particular, the first 5 are audio files, next 3 are text files, and final 6 are video files."""

all_files = []
all_summaries = []

for modality_type in ["audio", "text", "video"]:
  files, summaries = create_summary(modality_type)
  all_files.extend(files)
  all_summaries.extend(summaries)

db = create_chroma_db(all_summaries, "nasa")

"""Again, ensure that the embeddings were generated. Notice that now, we have audio, video, and text data."""

data = {
    'embeddings': db.peek()['embeddings'],
    'documents': db.peek()['documents']
}

df = pd.DataFrame.from_dict(data, orient='index').transpose()
df

files = get_relevant_files("communication with Mission Control", db)
print(files)

"""Can we do more than just return the most relevant file? Yes we can! We can ask Gemini to return a response to the query using the files it thinks are most relevant, provide an answer and tell us what files it used! This is really exciting, and has vast applications in many industries."""

def query_rag(query, db):
    files = get_relevant_files(query, db)
    # Ensure all_files is a dictionary mapping file identifiers to content
    all_files = {f: "content for file " + f for f in files} # Replace with actual content retrieval
    prompt = [all_files.get(f, "") for f in files]
    prompt.append("Generate a response to the query using the provided files. Here is the query.")
    prompt.append(query)
    # Ensure all_file_names is a dictionary
    all_file_names = {f: "name for file " + f for f in files} # Replace with actual name retrieval
    return model.generate_content(prompt).text, [all_file_names.get(f, "") for f in files]

for response in query_rag("Explain what happened with the Apollo 11 Mission.", db):
    print(response)

for response in query_rag("What happens at the Translunar Coast in the Mission Description?", db):
    print(response)

for response in query_rag("REPLACE ME: Ask any questions you'd like here about Apollo 11!", db):
    print(response)

"""Congrats! You've built a full end to end multimodal RAG with just a few tools. We hope you enjoyed following along in this notebook and learned a lot on the way."""